{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPxIg0yhV4JjPsIKVubfJRb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OJB-Quantum/Notebooks-for-Ideas/blob/main/Mistral_LLM_Usage_in_Colab_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Install and run Ollama on Google Colab with an NVIDIA L4 or A100 GPU.\n",
        "\n",
        "This notebook-style script:\n",
        "  1) Detects the active GPU (expects L4 or A100, continues otherwise).\n",
        "  2) Installs dependencies and Ollama (Linux).\n",
        "  3) Starts `ollama serve` in the background.\n",
        "  4) Pulls the `mistral-nemo` model.\n",
        "  5) Runs a minimal chat request using `from ollama import chat`.\n",
        "\n",
        "References (documentation-level):\n",
        "  - Ollama Linux install and manual install guidance.\n",
        "  - Ollama hardware support (NVIDIA, CUDA_VISIBLE_DEVICES).\n",
        "  - Ollama API version endpoint for health checks.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import socket\n",
        "import subprocess\n",
        "import time\n",
        "import urllib.error\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Optional, Sequence\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ColabOllamaConfig:\n",
        "    \"\"\"Configuration for installing and running Ollama in Colab.\n",
        "\n",
        "    Args:\n",
        "        model_name: The Ollama model tag to pull and run.\n",
        "        ollama_host: Host interface to bind the Ollama server.\n",
        "        ollama_port: TCP port for the Ollama server.\n",
        "        models_dir: Directory where Ollama will store downloaded models.\n",
        "        log_path: File path for Ollama server logs.\n",
        "        cuda_visible_devices: CUDA device selection string, for multi-GPU cases.\n",
        "        num_ctx: Optional context window size to request per chat call.\n",
        "        install_ollama: Whether to install Ollama (set False if already present).\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = \"mistral-nemo\"\n",
        "    ollama_host: str = \"127.0.0.1\"\n",
        "    ollama_port: int = 11434\n",
        "    models_dir: Path = Path(\"/content/ollama_models\")\n",
        "    log_path: Path = Path(\"/content/ollama_serve.log\")\n",
        "    cuda_visible_devices: str = \"0\"\n",
        "    num_ctx: int = 8192\n",
        "    install_ollama: bool = True\n",
        "\n",
        "\n",
        "CFG = ColabOllamaConfig()\n",
        "\n",
        "\n",
        "def _is_root() -> bool:\n",
        "    \"\"\"Return True if the current process has root privileges.\"\"\"\n",
        "    try:\n",
        "        return os.geteuid() == 0\n",
        "    except AttributeError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def _sudo_prefix() -> str:\n",
        "    \"\"\"Return 'sudo ' when needed, otherwise ''.\"\"\"\n",
        "    return \"\" if _is_root() else \"sudo \"\n",
        "\n",
        "\n",
        "def run_bash(command: str, *, check: bool = True) -> subprocess.CompletedProcess:\n",
        "    \"\"\"Run a bash command in a Colab-friendly way.\n",
        "\n",
        "    Args:\n",
        "        command: Command string to run via `bash -lc`.\n",
        "        check: When True, raise CalledProcessError on non-zero exit.\n",
        "\n",
        "    Returns:\n",
        "        CompletedProcess containing return code and execution metadata.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[run] {command}\\n\")\n",
        "    return subprocess.run(\n",
        "        [\"bash\", \"-lc\", command],\n",
        "        check=check,\n",
        "    )\n",
        "\n",
        "\n",
        "def capture_bash(command: str) -> str:\n",
        "    \"\"\"Run a bash command and capture stdout as text.\n",
        "\n",
        "    Args:\n",
        "        command: Command string to run via `bash -lc`.\n",
        "\n",
        "    Returns:\n",
        "        Standard output, as a stripped string.\n",
        "    \"\"\"\n",
        "    out = subprocess.check_output([\"bash\", \"-lc\", command], text=True)\n",
        "    return out.strip()\n",
        "\n",
        "\n",
        "def is_tcp_port_open(host: str, port: int, timeout_s: float = 0.25) -> bool:\n",
        "    \"\"\"Return True if a TCP port is accepting connections.\n",
        "\n",
        "    Args:\n",
        "        host: Hostname or IP address.\n",
        "        port: TCP port number.\n",
        "        timeout_s: Socket timeout in seconds.\n",
        "\n",
        "    Returns:\n",
        "        True if connect() succeeds, otherwise False.\n",
        "    \"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
        "        sock.settimeout(timeout_s)\n",
        "        return sock.connect_ex((host, port)) == 0\n",
        "\n",
        "\n",
        "def wait_for_ollama_ready(\n",
        "    host: str,\n",
        "    port: int,\n",
        "    timeout_s: float = 30.0,\n",
        "    poll_s: float = 0.5,\n",
        ") -> dict:\n",
        "    \"\"\"Wait until the Ollama server responds to GET /api/version.\n",
        "\n",
        "    Args:\n",
        "        host: Ollama server host.\n",
        "        port: Ollama server port.\n",
        "        timeout_s: Max time to wait for readiness.\n",
        "        poll_s: Poll interval.\n",
        "\n",
        "    Returns:\n",
        "        Parsed JSON response from /api/version.\n",
        "\n",
        "    Raises:\n",
        "        TimeoutError: If the server does not become ready in time.\n",
        "    \"\"\"\n",
        "    url = f\"http://{host}:{port}/api/version\"\n",
        "    deadline = time.time() + timeout_s\n",
        "    last_err: Optional[Exception] = None\n",
        "\n",
        "    while time.time() < deadline:\n",
        "        try:\n",
        "            with urllib.request.urlopen(url, timeout=2.0) as resp:\n",
        "                payload = resp.read().decode(\"utf-8\")\n",
        "            return json.loads(payload)\n",
        "        except (urllib.error.URLError, json.JSONDecodeError) as err:\n",
        "            last_err = err\n",
        "            time.sleep(poll_s)\n",
        "\n",
        "    raise TimeoutError(f\"Ollama did not become ready: {last_err}\")\n",
        "\n",
        "\n",
        "def nvidia_smi_summary() -> str:\n",
        "    \"\"\"Return a concise GPU summary from nvidia-smi, if available.\"\"\"\n",
        "    if subprocess.call([\"bash\", \"-lc\", \"command -v nvidia-smi >/dev/null 2>&1\"]) != 0:\n",
        "        return \"nvidia-smi not found (GPU runtime may be disabled).\"\n",
        "    query = (\n",
        "        \"nvidia-smi --query-gpu=name,driver_version,memory.total \"\n",
        "        \"--format=csv,noheader\"\n",
        "    )\n",
        "    return capture_bash(query)\n",
        "\n",
        "\n",
        "def assert_or_warn_l4_a100(gpu_summary: str) -> None:\n",
        "    \"\"\"Warn if the GPU does not look like an L4 or A100.\n",
        "\n",
        "    Args:\n",
        "        gpu_summary: Output line(s) from nvidia_smi_summary().\n",
        "    \"\"\"\n",
        "    upper = gpu_summary.upper()\n",
        "    is_expected = (\" L4\" in upper) or (\"A100\" in upper)\n",
        "    if is_expected:\n",
        "        print(f\"[gpu] Detected supported target GPU family: {gpu_summary}\")\n",
        "        return\n",
        "\n",
        "    print(\n",
        "        \"[gpu] WARNING: This runtime does not look like an L4 or A100.\\n\"\n",
        "        f\"      Detected: {gpu_summary}\\n\"\n",
        "        \"      The notebook will continue, since Ollama can still run on other \"\n",
        "        \"      NVIDIA GPUs, although performance and capacity will differ.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "7HCbZ3kJhzom"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TdTkHaahgPC",
        "outputId": "3daeaa1a-a22f-4817-d159-2564d70e6637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA L4, 550.54.15, 23034 MiB\n",
            "[gpu] Detected supported target GPU family: NVIDIA L4, 550.54.15, 23034 MiB\n",
            "\n",
            "[run] df -h /content\n",
            "\n",
            "\n",
            "[run] uname -a\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['bash', '-lc', 'uname -a'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# GPU / driver check (Colab runtime must be set to GPU).\n",
        "gpu = nvidia_smi_summary()\n",
        "print(gpu)\n",
        "assert_or_warn_l4_a100(gpu)\n",
        "\n",
        "# Disk sanity (models can be multiple gigabytes).\n",
        "run_bash(\"df -h /content\")\n",
        "run_bash(\"uname -a\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a few baseline tools. Ollama's installer may require standard\n",
        "# utilities, and (depending on packaging) zstd can matter for extraction.\n",
        "sudo = _sudo_prefix()\n",
        "\n",
        "if CFG.install_ollama:\n",
        "    run_bash(f\"{sudo}apt-get update -y\")\n",
        "    run_bash(\n",
        "        f\"{sudo}apt-get install -y \"\n",
        "        \"curl ca-certificates zstd pciutils lshw\"\n",
        "    )\n",
        "\n",
        "    # Official install script (Linux).\n",
        "    # This installs Ollama and its libraries, and it may attempt systemd setup,\n",
        "    # which is typically inactive inside Colab (that is expected).\n",
        "    run_bash(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "\n",
        "# Verify installation.\n",
        "run_bash(\"command -v ollama\")\n",
        "run_bash(\"ollama -v\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tejoba_FxskC",
        "outputId": "0f242f33-81ca-4978-8207-1bbf8833611c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[run] apt-get update -y\n",
            "\n",
            "\n",
            "[run] apt-get install -y curl ca-certificates zstd pciutils lshw\n",
            "\n",
            "\n",
            "[run] curl -fsSL https://ollama.com/install.sh | sh\n",
            "\n",
            "\n",
            "[run] command -v ollama\n",
            "\n",
            "\n",
            "[run] ollama -v\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['bash', '-lc', 'ollama -v'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure models directory exists (and is writable).\n",
        "CFG.models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# If a server is already bound, do not start a second one.\n",
        "if is_tcp_port_open(CFG.ollama_host, CFG.ollama_port):\n",
        "    print(\n",
        "        f\"[ollama] Server already listening on \"\n",
        "        f\"{CFG.ollama_host}:{CFG.ollama_port}\"\n",
        "    )\n",
        "else:\n",
        "    env = os.environ.copy()\n",
        "    env[\"OLLAMA_HOST\"] = f\"{CFG.ollama_host}:{CFG.ollama_port}\"\n",
        "    env[\"OLLAMA_MODELS\"] = str(CFG.models_dir)\n",
        "\n",
        "    # For completeness, and for multi-GPU environments, Ollama docs recommend\n",
        "    # CUDA_VISIBLE_DEVICES for selecting NVIDIA GPUs.\n",
        "    env[\"CUDA_VISIBLE_DEVICES\"] = CFG.cuda_visible_devices\n",
        "\n",
        "    CFG.log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[ollama] Logging to: {CFG.log_path}\")\n",
        "\n",
        "    with CFG.log_path.open(\"a\", encoding=\"utf-8\") as log_file:\n",
        "        proc = subprocess.Popen(\n",
        "            [\"ollama\", \"serve\"],\n",
        "            env=env,\n",
        "            stdout=log_file,\n",
        "            stderr=subprocess.STDOUT,\n",
        "        )\n",
        "\n",
        "    print(f\"[ollama] Started server PID={proc.pid}\")\n",
        "\n",
        "# Health check via documented endpoint: GET /api/version\n",
        "version_payload = wait_for_ollama_ready(CFG.ollama_host, CFG.ollama_port)\n",
        "print(\"[ollama] /api/version =>\", version_payload)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghyjv_NOxu_b",
        "outputId": "92d94508-5b6a-4ccc-8de2-84d3cc398b9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ollama] Logging to: /content/ollama_serve.log\n",
            "[ollama] Started server PID=4220\n",
            "[ollama] /api/version => {'version': '0.14.3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the model (first-time download can take a while and uses disk).\n",
        "run_bash(f\"ollama pull {CFG.model_name}\")\n",
        "\n",
        "# Install the official Python client library for Ollama.\n",
        "run_bash(\"python -m pip install -U pip\")\n",
        "run_bash(\"python -m pip install -U ollama\")\n",
        "\n",
        "# Optional: show installed models.\n",
        "run_bash(\"ollama list\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF5AvlzBxxGd",
        "outputId": "592a8d82-0122-41c3-ae70-23de215f470a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[run] ollama pull mistral-nemo\n",
            "\n",
            "\n",
            "[run] python -m pip install -U pip\n",
            "\n",
            "\n",
            "[run] python -m pip install -U ollama\n",
            "\n",
            "\n",
            "[run] ollama list\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['bash', '-lc', 'ollama list'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ollama import chat\n",
        "\n",
        "response = chat(\n",
        "    model=\"mistral-nemo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
        "    # If you want to explicitly request a larger context window:\n",
        "    # options={\"num_ctx\": CFG.num_ctx},\n",
        ")\n",
        "print(response.message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZZPc0e9xxD5",
        "outputId": "56d7a270-e752-40ca-fb40-2d91dd2db803"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! How can I assist you today? Let's chat about anything you'd like. ðŸ˜Š\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ollama provides `ollama ps` to show whether a model is loaded on GPU/CPU.\n",
        "run_bash(\"ollama ps\")\n",
        "\n",
        "# Also verify via NVIDIA telemetry. You can rerun this during generation.\n",
        "run_bash(\"nvidia-smi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI8V2vhAxxBY",
        "outputId": "2849d7c0-a44c-4b0d-8510-52f43b5fd73d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[run] ollama ps\n",
            "\n",
            "\n",
            "[run] nvidia-smi\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['bash', '-lc', 'nvidia-smi'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Interactive prompting loop for Ollama chat in Google Colab.\n",
        "\n",
        "This cell creates a minimal REPL that:\n",
        "  1) Reads your prompt via input().\n",
        "  2) Sends it to Ollama using `from ollama import chat`.\n",
        "  3) Prints the assistant response.\n",
        "  4) Preserves conversation state across turns.\n",
        "\n",
        "To stop the loop:\n",
        "  - Type: quit\n",
        "  - Or click the Colab stop button for the running cell.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "from ollama import chat\n",
        "\n",
        "# Control knobs (edit these freely).\n",
        "MODEL_NAME = \"mistral-nemo\"\n",
        "NUM_CTX = 8192  # Context tokens, higher uses more VRAM (Video Random Access Memory).\n",
        "SYSTEM_PROMPT = \"\"  # Example: \"You are a precise technical assistant.\"\n",
        "\n",
        "# Conversation state.\n",
        "messages: List[Dict[str, str]] = []\n",
        "if SYSTEM_PROMPT.strip():\n",
        "    messages.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT.strip()})\n",
        "\n",
        "\n",
        "def send_turn(user_text: str) -> str:\n",
        "    \"\"\"Send one user turn, return one assistant turn.\n",
        "\n",
        "    Args:\n",
        "        user_text: The user's prompt.\n",
        "\n",
        "    Returns:\n",
        "        The assistant's response text.\n",
        "    \"\"\"\n",
        "    messages.append({\"role\": \"user\", \"content\": user_text})\n",
        "    response = chat(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        options={\"num_ctx\": NUM_CTX},\n",
        "    )\n",
        "    assistant_text = response.message.content\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_text})\n",
        "    return assistant_text\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_text = input(\"\\nYou: \").strip()\n",
        "    if user_text.lower() in {\"quit\", \"exit\", \"q\"}:\n",
        "        print(\"Stopped.\")\n",
        "        break\n",
        "    if not user_text:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        reply = send_turn(user_text)\n",
        "    except Exception as exc:  # pylint: disable=broad-except\n",
        "        raise RuntimeError(\n",
        "            \"Prompting failed. Verify these in earlier cells:\\n\"\n",
        "            \"  1) `ollama serve` is running (and reachable on localhost:11434)\\n\"\n",
        "            \"  2) the model is present (`ollama pull mistral-nemo`)\\n\"\n",
        "            \"  3) the `ollama` Python package is installed (`pip install ollama`)\\n\"\n",
        "        ) from exc\n",
        "\n",
        "    print(f\"\\nAssistant: {reply}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s0tEjzKHOyi",
        "outputId": "521df788-3432-43a6-f8cd-d1445d4896d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You: Write a rigorous symbolic mathematical derivation of the averaged momentum equation. Then provide two numerical examples with realistic values and then show the results in scientific e-notation rounded to 4 decimal places. \n",
            "\n",
            "Assistant: **Derivation of the Averaged Momentum Equation:**\n",
            "\n",
            "Consider a control volume with inflow and outflow velocities, \\(V_{in}\\) and \\(V_{out}\\), respectively. The mass flow rates at the inlet and outlet are \\(\\dot{m}_{in} = \\rho V_{in}A\\) and \\(\\dot{m}_{out} = \\rho V_{out}A\\), where \\(\\rho\\) is the fluid density and \\(A\\) is the cross-sectional area of the control volume.\n",
            "\n",
            "The momentum entering the control volume per unit time is \\((\\rho V_{in}^2)A + (\\rho g)(\\frac{1}{2})V_{in}tA\\), where \\(g\\) is the acceleration due to gravity, \\(t\\) is the thickness of the boundary layer, and we've assumed the inlet flow is perpendicular to the control volume's surface.\n",
            "\n",
            "The momentum leaving the control volume per unit time is \\((\\rho V_{out}^2)A + (\\rho g)(\\frac{1}{2})V_{out}tA\\).\n",
            "\n",
            "The net change in momentum per unit time due to forces acting on the control volume, including pressure force \\(F_p\\) and friction force \\(F_f\\), is \\(F_p - F_f\\).\n",
            "\n",
            "The averaged momentum equation, considering a unit depth for simplicity, can be derived as:\n",
            "\n",
            "\\[\n",
            "(\\rho V_{out}^2)A + (\\rho g)(\\frac{1}{2})V_{out}tA - (\\rho V_{in}^2)A - (\\rho g)(\\frac{1}{2})V_{in}tA = F_p - F_f\n",
            "\\]\n",
            "\n",
            "Simplifying, we get:\n",
            "\n",
            "\\[\n",
            "\\rho A(V_{out}V_{out} + \\frac{g}{2}V_{out}t - V_{in}V_{in} - \\frac{g}{2}V_{in}t) = F_p - F_f\n",
            "\\]\n",
            "\n",
            "Dividing through by \\(A\\) and rearranging terms gives the averaged momentum equation:\n",
            "\n",
            "\\[\n",
            "\\rho(V_{out}^2 - V_{in}^2 + gt(V_{out} - V_{in})) = F_p - F_f\n",
            "\\]\n",
            "\n",
            "**Numerical Examples:**\n",
            "\n",
            "1. **Example with Air at STP:**\n",
            "\n",
            "   Given:\n",
            "   \\(V_{in} = 5\\) m/s, \\(V_{out} = 3\\) m/s, \\(\\rho = 1.225\\) kg/mÂ³, \\(g = 9.81\\) m/sÂ², \\(t = 0.01\\) m, \\(F_p = 500\\) N, \\(F_f = 300\\) N.\n",
            "\n",
            "   Plugging in the values:\n",
            "\n",
            "   \\[\n",
            "   \\rho(V_{out}^2 - V_{in}^2 + gt(V_{out} - V_{in})) = F_p - F_f\n",
            "   \\]\n",
            "\n",
            "   \\[\n",
            "   1.225(3^2 - 5^2 + 9.81*0.01*(3-5)) = 500 - 300\n",
            "   \\]\n",
            "\n",
            "   \\[\n",
            "   1.225(-4 + (-14.7)) = 200\n",
            "   \\]\n",
            "\n",
            "   \\[\n",
            "   -61.5 = 200\n",
            "   \\]\n",
            "\n",
            "   Scientific notation: \\(-61.5\\) is already in decimal notation, so no change.\n",
            "\n",
            "2. **Example with Water:**\n",
            "\n",
            "   Given:\n",
            "   \\(V_{in} = 3\\) m/s, \\(V_{out} = 1.5\\) m/s, \\(\\rho = 1000\\) kg/mÂ³, \\(g = 9.81\\) m/sÂ², \\(t = 0.02\\) m, \\(F_p = 8000\\) N, \\(F_f = 4000\\) N.\n",
            "\n",
            "   Plugging in the values:\n",
            "\n",
            "   \\[\n",
            "   \\rho(V_{out}^2 - V_{in}^2 + gt(V_{out} - V_{in})) = F_p - F_f\n",
            "   \\]\n",
            "\n",
            "   \\[\n",
            "   1000(1.5^2 - 3^2 + 9.81*0.02*(1.5-3)) = 8000 - 4000\n",
            "   \\]\n",
            "\n",
            "   \\[\n",
            "   1000(-4 + (-6.7)) = 4000\n",
            "   \\]\n",
            "\n",
            "   \\[\n",
            "   -10700 = 4000\n",
            "   \\]\n",
            "\n",
            "   Scientific notation: \\(-10700\\) can be expressed as \\(-1.07 \\times 10^4\\). Rounded to 4 decimal places, it's \\(-1.0698 \\times 10^4\\).\n",
            "\n",
            "**Results:**\n",
            "\n",
            "- For air at STP: The averaged momentum equation results in a net force of \\(200\\) N acting on the control volume.\n",
            "- For water: The averaged momentum equation results in a net force of \\(-1.07 \\times 10^4\\) N (or \\(-1.0698 \\times 10^4\\) N when rounded to 4 decimal places) acting on the control volume.\n",
            "\n",
            "You: Now show the script as a raw LaTeX script.\n",
            "\n",
            "Assistant: Here is the LaTeX script for the derivation and numerical examples:\n",
            "\n",
            "```latex\n",
            "\\documentclass{article}\n",
            "\\usepackage{amsmath}\n",
            "\\usepackage{siunitx}\n",
            "\\begin{document}\n",
            "\n",
            "\\section*{Derivation of Averaged Momentum Equation}\n",
            "Consider a control volume with inflow and outflow velocities, \\(V_{in}\\) and \\(V_{out}\\), respectively. The mass flow rates at the inlet and outlet are \\(\\dot{m}_{in} = \\rho V_{in}A\\) and \\(\\dot{m}_{out} = \\rho V_{out}A\\), where \\(\\rho\\) is the fluid density, \\(A\\) is the cross-sectional area of the control volume.\n",
            "\n",
            "The momentum entering the control volume per unit time is \\((\\rho V_{in}^2)A + (\\rho g)(\\frac{1}{2})V_{in}tA\\), and the momentum leaving the control volume per unit time is \\((\\rho V_{out}^2)A + (\\rho g)(\\frac{1}{2})V_{out}tA\\).\n",
            "\n",
            "The net change in momentum per unit time due to forces acting on the control volume, including pressure force \\(F_p\\) and friction force \\(F_f\\), is \\(F_p - F_f\\).\n",
            "\n",
            "The averaged momentum equation, considering a unit depth for simplicity, can be derived as:\n",
            "\\begin{align*}\n",
            "(\\rho V_{out}^2)A + (\\rho g)(\\frac{1}{2})V_{out}tA &- (\\rho V_{in}^2)A - (\\rho g)(\\frac{1}{2})V_{in}tA \\\\\n",
            "&= F_p - F_f\n",
            "\\end{align*}\n",
            "\n",
            "Simplifying, we get:\n",
            "\\begin{align*}\n",
            "\\rho A(V_{out}V_{out} + \\frac{g}{2}V_{out}t &- V_{in}V_{in} - \\frac{g}{2}V_{in}t) \\\\\n",
            "&= F_p - F_f\n",
            "\\end{align*}\n",
            "\n",
            "Dividing through by \\(A\\) and rearranging terms gives the averaged momentum equation:\n",
            "\\begin{equation}\n",
            "\\rho(V_{out}^2 - V_{in}^2 + gt(V_{out} - V_{in})) = F_p - F_f \\label{eq:momentum_eq}\n",
            "\\end{equation}\n",
            "\n",
            "\\section*{Numerical Examples}\n",
            "\\subsection*{Example 1: Air at Standard Temperature and Pressure (STP)}\n",
            "Given:\n",
            "\\begin{align*}\n",
            "V_{in} &= 5\\si{\\meter\\per\\second}, \\\\\n",
            "V_{out} &= 3\\si{\\meter\\per\\second}, \\\\\n",
            "\\rho &= 1.225\\si{\\kilogram\\per\\cubic\\meter}, \\\\\n",
            "g &= 9.81\\si{\\meter\\per\\square\\second}, \\\\\n",
            "t &= 0.01\\si{\\meter}, \\\\\n",
            "F_p &= 500\\si{\\newton}, \\\\\n",
            "F_f &= 300\\si{\\newton}.\n",
            "\\end{align*}\n",
            "\n",
            "Plugging in the values into Equation \\ref{eq:momentum_eq}:\n",
            "\\begin{align*}\n",
            "\\rho(V_{out}^2 - V_{in}^2 + gt(V_{out} - V_{in})) & = F_p - F_f \\\\\n",
            "1.225(3^2 - 5^2 + 9.81*0.01*(3-5)) &= 500 - 300 \\\\\n",
            "-61.5 &= 200\n",
            "\\end{align*}\n",
            "The result is a net force of \\SI{200}{\\newton} acting on the control volume.\n",
            "\n",
            "\\subsection*{Example 2: Water}\n",
            "Given:\n",
            "\\begin{align*}\n",
            "V_{in} &= 3\\si{\\meter\\per\\second}, \\\\\n",
            "V_{out} &= 1.5\\si{\\meter\\per\\second}, \\\\\n",
            "\\rho &= 1000\\si{\\kilogram\\per\\cubic\\meter}, \\\\\n",
            "g &= 9.81\\si{\\meter\\per\\square\\second}, \\\\\n",
            "t &= 0.02\\si{\\meter}, \\\\\n",
            "F_p &= 8000\\si{\\newton}, \\\\\n",
            "F_f &= 4000\\si{\\newton}.\n",
            "\\end{align*}\n",
            "\n",
            "Plugging in the values into Equation \\ref{eq:momentum_eq}:\n",
            "\\begin{align*}\n",
            "\\rho(V_{out}^2 - V_{in}^2 + gt(V_{out} - V_{in})) & = F_p - F_f \\\\\n",
            "1000(1.5^2 - 3^2 + 9.81*0.02*(1.5-3)) &= 8000 - 4000 \\\\\n",
            "-10700 &\\approx -1.07 \\times 10^4\n",
            "\\end{align*}\n",
            "The result is a net force of approximately $-1.07 \\times 10^4\\si{\\newton}$ acting on the control volume.\n",
            "\n",
            "\\end{document}\n",
            "```\n",
            "\n",
            "You: Quit\n",
            "Stopped.\n"
          ]
        }
      ]
    }
  ]
}